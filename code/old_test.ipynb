{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/huang/anaconda3/envs/steer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/export/home/huang/anaconda3/envs/steer/lib/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `PYTORCH_PRETRAINED_BERT_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/export/home/huang/anaconda3/envs/steer/lib/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `PYTORCH_TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/export/home/huang/anaconda3/envs/steer/lib/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from dialz import Dataset, SteeringModel, SteeringVector, get_activation_score\n",
    "from score import get_unaggregated_activation_score\n",
    "from models import LinearClassifier, TransformerClassifier, CustomDataset, optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes indeed. She sort of reminds me of the elder lady that played the part in the movie \"Titanic\" who was telling her story!!! And I wouldn't have wanted to cover who I really am!! I would be proud!!!! WE should be proud of our race no matter what it is!!\n",
      "-3.9\n",
      "The trans women reading this tweet right now is beautiful\n",
      "-6.52\n",
      "Question: These 4 broads who criticize America, what country did they flee to get here? And now they want to make OUR America like THEIR former HELL HOLE. I don't think so!!!!!!!!!!  Let them explain their GRATITUDE for letting them in OUR country.\n",
      "0.36\n",
      "It is about time for all illegals to go back to their country of origin and keep our freeway open and prevent heavy traffic.\n",
      "0.26\n",
      "For starters bend over the one in pink and kick that ass and pussy to get a taste until she's begging for a dick inside her.\n",
      "1.54\n"
     ]
    }
   ],
   "source": [
    "# Hate Speech Dataset Load\n",
    "df = pd.read_parquet(\"hf://datasets/ucberkeley-dlab/measuring-hate-speech/measuring-hate-speech.parquet\")\n",
    "df = df[['text', 'hate_speech_score', 'comment_id', 'annotator_id']]\n",
    "df.head()\n",
    "\n",
    "texts = df.head()['text'].to_list()\n",
    "scores = df.head()['hate_speech_score'].to_list()\n",
    "for i, text in enumerate(texts):\n",
    "    print(text)\n",
    "    print(scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 78427\n",
      "Number of label 0: 37856 (48.27%)\n",
      "Number of label 1: 40571 (51.73%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hate_speech_score</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes indeed. She sort of reminds me of the elde...</td>\n",
       "      <td>-3.90</td>\n",
       "      <td>47777</td>\n",
       "      <td>10873</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The trans women reading this tweet right now i...</td>\n",
       "      <td>-6.52</td>\n",
       "      <td>39773</td>\n",
       "      <td>2790</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For starters bend over the one in pink and kic...</td>\n",
       "      <td>1.54</td>\n",
       "      <td>12538</td>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sounds like the kinda wholsesome life I'd die ...</td>\n",
       "      <td>-4.93</td>\n",
       "      <td>11001</td>\n",
       "      <td>527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fuck off you insufferable retarded faggot.</td>\n",
       "      <td>2.08</td>\n",
       "      <td>17034</td>\n",
       "      <td>6801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  hate_speech_score  \\\n",
       "0  Yes indeed. She sort of reminds me of the elde...              -3.90   \n",
       "1  The trans women reading this tweet right now i...              -6.52   \n",
       "4  For starters bend over the one in pink and kic...               1.54   \n",
       "5  Sounds like the kinda wholsesome life I'd die ...              -4.93   \n",
       "7         Fuck off you insufferable retarded faggot.               2.08   \n",
       "\n",
       "   comment_id  annotator_id  label  \n",
       "0       47777         10873      0  \n",
       "1       39773          2790      0  \n",
       "4       12538           488      1  \n",
       "5       11001           527      0  \n",
       "7       17034          6801      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column 'label' based on the hate speech score\n",
    "df['label'] = df['hate_speech_score'].apply(lambda x: 1 if x > 1 else (0 if x < -2 else None))\n",
    "\n",
    "# Remove rows without a label\n",
    "df = df.dropna(subset=['label'])\n",
    "\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Print the head of the dataframe and the number of rows\n",
    "print(f\"Number of rows: {len(df)}\")\n",
    "label_counts = df['label'].value_counts()\n",
    "label_percentages = df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"Number of label 0: {label_counts[0]} ({label_percentages[0]:.2f}%)\")\n",
    "print(f\"Number of label 1: {label_counts[1]} ({label_percentages[1]:.2f}%)\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "## Steering vector\n",
    "def compute_activation_score(\n",
    "        df: pd.DataFrame,\n",
    "        scoring_method: str,\n",
    "        model_name: str, \n",
    "        items: list, \n",
    "        prompt_type: str, \n",
    "        num_sents: int,\n",
    "        system_role: str,\n",
    "    ):\n",
    "    dataset = Dataset.create_dataset(model_name, items, prompt_type=prompt_type, num_sents=num_sents, system_role=system_role)\n",
    "    \n",
    "    model = SteeringModel(model_name, list(range(-5, -18, -1)), hf_token)\n",
    "    vector = SteeringVector.train(model, dataset)\n",
    "    max_token_length = 0\n",
    "    activation_scores = []\n",
    "    unaggregated_activation_scores = []\n",
    "\n",
    "    for i, text in tqdm(enumerate(df['text']), total=len(df), desc=\"Calculating activation scores\"):\n",
    "        activation_score, token_length, unaggregated_activation_score = get_unaggregated_activation_score(text, model, vector, layer_index=list(range(15, 20, 1)), \n",
    "                                                                                                          scoring_method=scoring_method)\n",
    "        activation_scores.append(activation_score)\n",
    "        unaggregated_activation_scores.append(unaggregated_activation_score)\n",
    "        max_token_length = max(max_token_length, token_length)\n",
    "        df.at[i, 'activation_score'] = activation_score\n",
    "        df.at[i, 'token_length'] = token_length\n",
    "    print(f\"Max token length: {max_token_length}\")\n",
    "\n",
    "    # Pad activation scores to max token length\n",
    "    print(\"Padding activation scores to max token length after tokenization.\")\n",
    "    padded_activation_scores = np.full((len(df.text), 5, max_token_length), 0, dtype=float)\n",
    "    for i, scores in enumerate(unaggregated_activation_scores):\n",
    "        for j, score in enumerate(scores):\n",
    "            length = len(score)\n",
    "            padded_activation_scores[i, j, :length] = score\n",
    "\n",
    "    return df, padded_activation_scores, max_token_length\n",
    "\n",
    "\n",
    "def run_classifier(\n",
    "        df: pd.DataFrame,\n",
    "        scoring_method: str,\n",
    "        model_name: str, \n",
    "        items: list, \n",
    "        prompt_type: str, \n",
    "        num_sents: int,\n",
    "        system_role: str,\n",
    "    ):\n",
    "    df, padded_activation_scores, max_token_length = compute_activation_score(df, scoring_method, model_name, items, prompt_type, num_sents, system_role)\n",
    "\n",
    "    # different classifiers\n",
    "    results = {}\n",
    "\n",
    "    # threshold-based classifier\n",
    "    def calculate_score(scores_df, label_0_condition, label_1_condition):\n",
    "        label_0_count = ((scores_df['label'] == 0) & label_0_condition).sum()\n",
    "        label_1_count = ((scores_df['label'] == 1) & label_1_condition).sum()\n",
    "        return label_0_count + label_1_count\n",
    "\n",
    "    # Generate a range of thresholds to test\n",
    "    thresholds = np.linspace(df['activation_score'].min(), df['activation_score'].max(), 1000)\n",
    "    condition = [calculate_score(df, df['activation_score'] < t, df['activation_score'] > t) for t in thresholds]\n",
    "    best_threshold = thresholds[np.argmax(condition)]\n",
    "    accuracy_condition = (np.max(condition) / len(df)) * 100\n",
    "    results['threshold_classifier'] = accuracy_condition\n",
    "    print(f\"Best threshold: {best_threshold}\")\n",
    "    print(f\"Accuracy: {accuracy_condition:.2f}%\")\n",
    "    \n",
    "    # Learning a linear classifier\n",
    "    acc = optimize(df, padded_activation_scores, max_token_length, learning_rate=1e-3, batch_size=64, epochs=50, is_transformer=False)\n",
    "    results['linear_classifier'] = acc\n",
    "    print(f\"Accuracy for linear layer: {acc:.2f}%\")\n",
    "\n",
    "    # Learning a transformer-based classifier\n",
    "    acc = optimize(df, padded_activation_scores, max_token_length, learning_rate=1e-3, batch_size=64, epochs=50, is_transformer=True)\n",
    "    results['transformer_classifier'] = acc\n",
    "    print(f\"Accuracy for transformer feature learner and then a linear classifier: {acc:.2f}%\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of label 1s: 54.60%\n",
      "Percentage of label 0s: 45.40%\n"
     ]
    }
   ],
   "source": [
    "df_2 = df.sample(n=500, random_state=42).reset_index(drop=True)\n",
    "\n",
    "label_1_percentage = (df_2['label'].value_counts(normalize=True)[1] * 100)\n",
    "label_0_percentage = (df_2['label'].value_counts(normalize=True)[0] * 100)\n",
    "\n",
    "print(f\"Percentage of label 1s: {label_1_percentage:.2f}%\")\n",
    "print(f\"Percentage of label 0s: {label_0_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.58it/s]\n",
      "100%|██████████| 25/25 [00:06<00:00,  4.06it/s]\n",
      "100%|██████████| 31/31 [00:02<00:00, 13.89it/s]\n",
      "Calculating activation scores: 100%|██████████| 500/500 [03:36<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token length: 172\n",
      "Padding activation scores to max token length after tokenization.\n",
      "500\n",
      "Best threshold: 1.427336531477767\n",
      "Accuracy: 86.80%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [500, 78427, 78427]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest_dataset_parameterized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_token\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhate speech\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mloving words\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentence-starters\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mYou are an example of how someone would respond with \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mtest_dataset_parameterized\u001b[39m\u001b[34m(df, scoring_method, model_name, items, prompt_type, num_sents, system_role)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_condition\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# MLP linear classifier\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m acc = \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_activation_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_token_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_transformer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# transformer classifier\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36moptimize\u001b[39m\u001b[34m(padded_activation_scores, max_token_length, learning_rate, batch_size, epochs, is_transformer)\u001b[39m\n\u001b[32m    105\u001b[39m     model = TransformerClassifier(max_token_length)\n\u001b[32m    106\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m X_train, X_test, text_train, text_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_activation_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m train_dataloader=CustomDataset(X_train, y_train, text_train)\n\u001b[32m    111\u001b[39m test_dataloader=CustomDataset(X_test, y_test, text_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/steer/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/steer/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2916\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_arrays == \u001b[32m0\u001b[39m:\n\u001b[32m   2914\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt least one array required as input\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2916\u001b[39m arrays = \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2918\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m   2919\u001b[39m n_train, n_test = _validate_shuffle_split(\n\u001b[32m   2920\u001b[39m     n_samples, test_size, train_size, default_test_size=\u001b[32m0.25\u001b[39m\n\u001b[32m   2921\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/steer/lib/python3.11/site-packages/sklearn/utils/validation.py:530\u001b[39m, in \u001b[36mindexable\u001b[39m\u001b[34m(*iterables)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[32m    501\u001b[39m \n\u001b[32m    502\u001b[39m \u001b[33;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m \u001b[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    529\u001b[39m result = [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/steer/lib/python3.11/site-packages/sklearn/utils/validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [500, 78427, 78427]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_dataset_parameterized(df_2, 'max_token', model_name, ['hate speech', 'loving words'], 'sentence-starters', 400, 'You are an example of how someone would respond with ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"prompt\": \"You are a crazy extremist who only responds with absolute \",\n",
    "# \"contrastive_words\": [\"hate\", \"love\"]\n",
    "# acc: 83.60%\n",
    "\n",
    "print('Test 2')\n",
    "test2 = test_dataset(df_2, 'mean', model_name, ['hate speech', 'loving words'], 'sentence-starters', 400, 'You are an example of how someone would respond with ')\n",
    "\n",
    "print('Test 3')\n",
    "test3 = test_dataset(df_2, 'final_token', model_name, ['hate speech', 'loving words'], 'sentence-starters', 400, 'You are an example of how someone would respond with ')\n",
    "\n",
    "print('Test 4')\n",
    "test4 = test_dataset(df_2, 'max_token', model_name, ['hate speech', 'loving words'], 'sentence-starters', 400, 'You are an example of how someone would respond with ')\n",
    "\n",
    "print('Test 5')\n",
    "test5 = test_dataset(df_2, 'median_token', model_name, ['hate speech', 'loving words'], 'sentence-starters', 400, 'You are an example of how someone would respond with ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then test scoring_methods default, last_token, max_token, median_token\n",
    "# test alla the different layer options\n",
    "\n",
    "methods = ['mean', 'final_token', 'max_token', 'median_token']\n",
    "setup = {\n",
    "    \"prompt\": \"You are a crazy extremist who only responds with absolute \",\n",
    "    \"contrastive_words\": [\"hate\", \"love\"]\n",
    "}\n",
    "\n",
    "for method in methods:\n",
    "    print(method)\n",
    "    test_dataset(df_2, method, model_name, setup[\"contrastive_words\"], 'starters', 400, setup[\"prompt\"])\n",
    "\n",
    "setup_2 = {\n",
    "    \"prompt\": \"You are an example of how someone would respond with \",\n",
    "    \"contrastive_words\": [\"hate speech\", \"loving words\"]\n",
    "}\n",
    "\n",
    "for method in methods:\n",
    "    print(method)\n",
    "    test_dataset(df_2, method, model_name, setup[\"contrastive_words\"], 'starters', 400, setup[\"prompt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = test4['activation_score'].median()\n",
    "max_value = test4['activation_score'].max()\n",
    "min_value = test4['activation_score'].min()\n",
    "\n",
    "print(f\"Median: {median}\")\n",
    "print(f\"Max: {max_value}\")\n",
    "print(f\"Min: {min_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.create_dataset(model_name, ['hate speech', 'loving words'], 'starters', 400, 'You are an example of how someone would respond with ')\n",
    "\n",
    "model = SteeringModel(model_name, list(range(-5, -18, -1)), hf_token)\n",
    "vector = SteeringVector.train(model, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter 10 texts with label 0 and 10 texts with label 1\n",
    "label_0_texts = df[df['label'] == 0].head(10)\n",
    "label_1_texts = df[df['label'] == 1].head(10)\n",
    "print(len(label_1_texts))\n",
    "# Process and print activation visualization and scores for label 0 texts\n",
    "print(\"Activation visualization and scores for label 0 texts:\")\n",
    "for i, row in label_0_texts.iterrows():\n",
    "    text = row['text']\n",
    "    activation_visualization = model.visualize_activation(input_text=text, control_vector=vector)\n",
    "    activation_score = get_activation_score(text, model, vector, layer_index=20)\n",
    "    print(f\"Row {i}:\\nText: {text}\\nActivation Visualization:\\n{activation_visualization}\\nActivation Score: {activation_score}\\n\")\n",
    "\n",
    "print(\"=====================================\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "# Process and print activation visualization and scores for label 1 texts\n",
    "print(\"Activation visualization and scores for label 1 texts:\")\n",
    "for i, row in label_1_texts.iterrows():\n",
    "    text = row['text']\n",
    "    activation_visualization = model.visualize_activation(input_text=text, control_vector=vector)\n",
    "    activation_score = get_activation_score(text, model, vector, layer_index=20)\n",
    "    print(f\"Row {i}:\\nText: {text}\\nActivation Visualization:\\n{activation_visualization}\\nActivation Score: {activation_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_strings = []\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=model.token)\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "def generate_with_vector(\n",
    "    input: str,\n",
    "    vector: SteeringVector,\n",
    "    coeffs: tuple[float, float],\n",
    "    max_new_tokens: int = 20,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    show_baseline: bool = True,\n",
    "):\n",
    "    positive_coeff, negative_coeff = coeffs\n",
    "    assert positive_coeff > 0\n",
    "    assert negative_coeff < 0\n",
    "\n",
    "    input_ids = tokenizer(input, return_tensors=\"pt\").to(model.device)\n",
    "    settings = {\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,  # silence warning\n",
    "        \"do_sample\": False,  # temperature=0\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "    }\n",
    "\n",
    "    if show_baseline:\n",
    "        print(\"==baseline ---------------------------------------------------\")\n",
    "        model.reset()\n",
    "        print(\n",
    "            tokenizer.decode(model.generate(**input_ids, **settings).squeeze()).strip()\n",
    "        )\n",
    "        list_of_strings.append(\n",
    "            tokenizer.decode(model.generate(**input_ids, **settings).squeeze()).strip()\n",
    "        )\n",
    "\n",
    "    print(\"\\n++control ---------------------------------------------------\")\n",
    "    model.set_control(vector, positive_coeff)\n",
    "    print(tokenizer.decode(model.generate(**input_ids, **settings).squeeze()).strip())\n",
    "    list_of_strings.append(\n",
    "        tokenizer.decode(model.generate(**input_ids, **settings).squeeze()).strip()\n",
    "    )\n",
    "\n",
    "    print(\"\\n--control ---------------------------------------------------\")\n",
    "    model.set_control(vector, negative_coeff)\n",
    "    print(tokenizer.decode(model.generate(**input_ids, **settings).squeeze()).strip())\n",
    "    list_of_strings.append(\n",
    "        tokenizer.decode(model.generate(**input_ids, **settings).squeeze()).strip()\n",
    "    )\n",
    "\n",
    "    model.reset()\n",
    "\n",
    "    with open(f\"output_{input[:10]}.txt\", \"w\") as f:\n",
    "        for string in list_of_strings:\n",
    "            f.write(string + \"\\n\")\n",
    "\n",
    "\n",
    "generate_with_vector(\"Write about a black man.\", vector, (2,-2), 60)\n",
    "\n",
    "\n",
    "\n",
    "# highlighted_output = model.visualize_activation(\n",
    "#     input_text=\"Good CEOs are all men. Men tend to be better at leadership roles.\",\n",
    "#     control_vector=vector,\n",
    "# )\n",
    "\n",
    "# print(highlighted_output)\n",
    "\n",
    "# highlighted_output = model.visualize_activation(\n",
    "#     input_text=\"Women are supposed to stay in the kitchen not go out and become nurses.\",\n",
    "#     control_vector=vector,\n",
    "# )\n",
    "# print(highlighted_output)\n",
    "\n",
    "# highlighted_output = model.visualize_activation(\n",
    "#     input_text=\"Mothers are the most amazing thing to bless humankind.\",\n",
    "#     control_vector=vector,\n",
    "# )\n",
    "# print(highlighted_output)\n",
    "\n",
    "# print(\n",
    "#     get_activation_score(\"Female empowerment is an amazing thing.\", model, vector)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "def classify_hate_speech(text):\n",
    "    try:\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4o\",\n",
    "            instructions=\"You are a helpful assistant that classifies text as hate speech or not.\",\n",
    "            input=f\"Is the following text hate speech? Respond only with 0 for no and 1 for yes: {text}\",\n",
    "        )\n",
    "        result = response.output_text\n",
    "        return int(result) if result in ['0', '1'] else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}\\nError: {e}\")\n",
    "        return None\n",
    "\n",
    "openai_results = df_2.copy()\n",
    "openai_results['openai'] = openai_results['text'].apply(classify_hate_speech)\n",
    "openai_results.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of None (missing) values in the predictions\n",
    "none_count = openai_results['openai'].isna().sum()\n",
    "print(f\"Number of None values in predictions: {none_count}\")\n",
    "\n",
    "# Filter out rows where the prediction is None\n",
    "valid_predictions = openai_results[openai_results['openai'].notna()]\n",
    "\n",
    "# Count how many predictions match the ground truth label\n",
    "correct_count = (valid_predictions['openai'] == valid_predictions['label']).sum()\n",
    "total_valid = len(valid_predictions)\n",
    "accuracy = (correct_count / total_valid) * 100 if total_valid > 0 else 0\n",
    "\n",
    "print(f\"Number of correctly classified texts: {correct_count} out of {total_valid}\")\n",
    "print(f\"Accuracy (excluding None values): {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows where the prediction is None\n",
    "none_rows = openai_results[openai_results['openai'].isna()]\n",
    "print(\"Texts with None predictions:\")\n",
    "for idx, row in none_rows.iterrows():\n",
    "    print(f\"Index {idx}: {row['text']}\")\n",
    "    \n",
    "# Rows where the prediction is not None but misclassified\n",
    "misclassified = openai_results[\n",
    "    (openai_results['openai'].notna()) &\n",
    "    (openai_results['openai'] != openai_results['label'])\n",
    "]\n",
    "print(\"\\nTexts with misclassified values:\")\n",
    "for idx, row in misclassified.iterrows():\n",
    "    print(f\"Index {idx}:\")\n",
    "    print(f\"Text: {row['text']}\")\n",
    "    print(f\"Ground Truth: {row['label']}, Prediction: {row['openai']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
